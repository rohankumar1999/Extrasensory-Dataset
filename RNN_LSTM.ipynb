{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resource : https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the extrasensory dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 users data.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, classification_report\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from utilize.data import *\n",
    "from utilize.transform import *\n",
    "from utilize.feature_selection import *\n",
    "from utilize.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(y_test, y_pred, score = 'BA', W_test = None):\n",
    "\n",
    "    mcm = []\n",
    "    for i in range(y_test.shape[1]):\n",
    "        if W_test is not None:\n",
    "            cm = confusion_matrix(y_test[:,i].T, y_pred[:,i].T, sample_weight = W_test[:,i].T)\n",
    "        else:\n",
    "            cm = confusion_matrix(y_test[:,i].T, y_pred[:,i].T)\n",
    "        cm = np.expand_dims(cm, axis = 0)\n",
    "        mcm.append(cm)\n",
    "    \n",
    "    mcm = np.concatenate(mcm, axis = 0)\n",
    "    tn = mcm[:, 0, 0]\n",
    "    tp = mcm[:, 1, 1]\n",
    "    fn = mcm[:, 1, 0]\n",
    "    fp = mcm[:, 0, 1]\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    BA = (sensitivity + specificity)/2\n",
    "    accuracy = (tn + tp)/(tn + tp + fn + fp)\n",
    "\n",
    "    sensitivity = np.sum(sensitivity)/sensitivity.shape[0]\n",
    "    specificity = np.sum(specificity)/specificity.shape[0]\n",
    "    BA = np.sum(BA)/BA.shape[0]\n",
    "    accuracy = np.sum(accuracy)/accuracy.shape[0]\n",
    "\n",
    "    if score == 'BA': \n",
    "        return BA\n",
    "    else: \n",
    "        raise Exception('score not valid!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data from Extrasenory dataset\n",
    "X, y, M, user_index, feature_names, label_names = load_all_data(['1155FF54-63D3-4AB2-9863-8385D0BD0A13'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = [ 0,   1,   2,   4,   6,   7,   9,  12,  13,  16,  17,  18,  19,\n",
    "        20,  22,  23,  24,  25,  26,  27,  28,  31,  32,  34,  35,  36,\n",
    "        37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  49,  52,  53,\n",
    "        54,  56,  58,  59,  60,  61,  62,  63,  65,  67,  68,  69,  71,\n",
    "        72,  73,  74,  76,  77,  78,  79,  80,  81,  85,  87,  88,  89,\n",
    "        90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
    "       103, 104, 105, 106, 107, 108, 109, 110, 111, 115, 116, 119, 120,\n",
    "       121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134,\n",
    "       135, 136, 137, 139, 140, 142, 144, 145, 146, 147, 148, 149, 150,\n",
    "       151, 153, 154, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166,\n",
    "       167, 168, 170, 171, 172, 173, 176, 177, 178, 180, 181, 182, 183,\n",
    "       185, 187, 188, 190, 192, 193, 194, 195, 196, 197, 198, 199, 201,\n",
    "       202, 203, 204, 205, 206, 207, 208, 209, 213, 214, 215, 216, 217,\n",
    "       218, 219, 220, 221, 222, 224]\n",
    "X175 = X[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-Domain Columns:\n",
      "raw_acc:magnitude_stats:mean\n",
      "raw_acc:magnitude_stats:std\n",
      "raw_acc:magnitude_stats:moment3\n",
      "raw_acc:magnitude_stats:moment4\n",
      "raw_acc:magnitude_stats:percentile25\n",
      "raw_acc:magnitude_stats:percentile50\n",
      "raw_acc:magnitude_stats:percentile75\n",
      "raw_acc:magnitude_stats:value_entropy\n",
      "raw_acc:magnitude_stats:time_entropy\n",
      "raw_acc:magnitude_spectrum:spectral_entropy\n",
      "raw_acc:3d:mean_x\n",
      "raw_acc:3d:mean_y\n",
      "raw_acc:3d:mean_z\n",
      "raw_acc:3d:std_x\n",
      "raw_acc:3d:std_y\n",
      "raw_acc:3d:std_z\n",
      "raw_acc:3d:ro_xy\n",
      "raw_acc:3d:ro_xz\n",
      "raw_acc:3d:ro_yz\n",
      "proc_gyro:magnitude_stats:mean\n",
      "proc_gyro:magnitude_stats:std\n",
      "proc_gyro:magnitude_stats:moment3\n",
      "proc_gyro:magnitude_stats:moment4\n",
      "proc_gyro:magnitude_stats:percentile25\n",
      "proc_gyro:magnitude_stats:percentile50\n",
      "proc_gyro:magnitude_stats:percentile75\n",
      "proc_gyro:magnitude_stats:value_entropy\n",
      "proc_gyro:magnitude_stats:time_entropy\n",
      "proc_gyro:magnitude_spectrum:spectral_entropy\n",
      "proc_gyro:3d:mean_x\n",
      "proc_gyro:3d:mean_y\n",
      "proc_gyro:3d:mean_z\n",
      "proc_gyro:3d:std_x\n",
      "proc_gyro:3d:std_y\n",
      "proc_gyro:3d:std_z\n",
      "proc_gyro:3d:ro_xy\n",
      "proc_gyro:3d:ro_xz\n",
      "proc_gyro:3d:ro_yz\n",
      "raw_magnet:magnitude_stats:mean\n",
      "raw_magnet:magnitude_stats:std\n",
      "raw_magnet:magnitude_stats:moment3\n",
      "raw_magnet:magnitude_stats:moment4\n",
      "raw_magnet:magnitude_stats:percentile25\n",
      "raw_magnet:magnitude_stats:percentile50\n",
      "raw_magnet:magnitude_stats:percentile75\n",
      "raw_magnet:magnitude_stats:value_entropy\n",
      "raw_magnet:magnitude_stats:time_entropy\n",
      "raw_magnet:magnitude_spectrum:spectral_entropy\n",
      "raw_magnet:3d:mean_x\n",
      "raw_magnet:3d:mean_y\n",
      "raw_magnet:3d:mean_z\n",
      "raw_magnet:3d:std_x\n",
      "raw_magnet:3d:std_y\n",
      "raw_magnet:3d:std_z\n",
      "raw_magnet:3d:ro_xy\n",
      "raw_magnet:3d:ro_xz\n",
      "raw_magnet:3d:ro_yz\n",
      "watch_acceleration:magnitude_stats:mean\n",
      "watch_acceleration:magnitude_stats:std\n",
      "watch_acceleration:magnitude_stats:moment3\n",
      "watch_acceleration:magnitude_stats:moment4\n",
      "watch_acceleration:magnitude_stats:percentile25\n",
      "watch_acceleration:magnitude_stats:percentile50\n",
      "watch_acceleration:magnitude_stats:percentile75\n",
      "watch_acceleration:magnitude_stats:value_entropy\n",
      "watch_acceleration:magnitude_stats:time_entropy\n",
      "watch_acceleration:magnitude_spectrum:spectral_entropy\n",
      "watch_acceleration:3d:mean_x\n",
      "watch_acceleration:3d:mean_y\n",
      "watch_acceleration:3d:mean_z\n",
      "watch_acceleration:3d:std_x\n",
      "watch_acceleration:3d:std_y\n",
      "watch_acceleration:3d:std_z\n",
      "watch_acceleration:3d:ro_xy\n",
      "watch_acceleration:3d:ro_xz\n",
      "watch_acceleration:3d:ro_yz\n",
      "watch_heading:mean_cos\n",
      "watch_heading:std_cos\n",
      "watch_heading:mean_sin\n",
      "watch_heading:std_sin\n",
      "watch_heading:entropy_8bins\n",
      "location_quick_features:std_lat\n",
      "location_quick_features:std_long\n",
      "location_quick_features:mean_abs_lat_deriv\n",
      "location_quick_features:mean_abs_long_deriv\n",
      "audio_naive:mfcc0:mean\n",
      "audio_naive:mfcc1:mean\n",
      "audio_naive:mfcc2:mean\n",
      "audio_naive:mfcc3:mean\n",
      "audio_naive:mfcc4:mean\n",
      "audio_naive:mfcc5:mean\n",
      "audio_naive:mfcc6:mean\n",
      "audio_naive:mfcc7:mean\n",
      "audio_naive:mfcc8:mean\n",
      "audio_naive:mfcc9:mean\n",
      "audio_naive:mfcc10:mean\n",
      "audio_naive:mfcc11:mean\n",
      "audio_naive:mfcc12:mean\n",
      "audio_naive:mfcc0:std\n",
      "audio_naive:mfcc1:std\n",
      "audio_naive:mfcc2:std\n",
      "audio_naive:mfcc3:std\n",
      "audio_naive:mfcc4:std\n",
      "audio_naive:mfcc5:std\n",
      "audio_naive:mfcc6:std\n",
      "audio_naive:mfcc7:std\n",
      "audio_naive:mfcc8:std\n",
      "audio_naive:mfcc9:std\n",
      "audio_naive:mfcc10:std\n",
      "audio_naive:mfcc11:std\n",
      "audio_naive:mfcc12:std\n",
      "\n",
      "Frequency-Domain Columns:\n",
      "raw_acc:magnitude_spectrum:log_energy_band0\n",
      "raw_acc:magnitude_spectrum:log_energy_band1\n",
      "raw_acc:magnitude_spectrum:log_energy_band2\n",
      "raw_acc:magnitude_spectrum:log_energy_band3\n",
      "raw_acc:magnitude_spectrum:log_energy_band4\n",
      "raw_acc:magnitude_spectrum:spectral_entropy\n",
      "proc_gyro:magnitude_spectrum:log_energy_band0\n",
      "proc_gyro:magnitude_spectrum:log_energy_band1\n",
      "proc_gyro:magnitude_spectrum:log_energy_band2\n",
      "proc_gyro:magnitude_spectrum:log_energy_band3\n",
      "proc_gyro:magnitude_spectrum:log_energy_band4\n",
      "proc_gyro:magnitude_spectrum:spectral_entropy\n",
      "raw_magnet:magnitude_spectrum:log_energy_band0\n",
      "raw_magnet:magnitude_spectrum:log_energy_band1\n",
      "raw_magnet:magnitude_spectrum:log_energy_band2\n",
      "raw_magnet:magnitude_spectrum:log_energy_band3\n",
      "raw_magnet:magnitude_spectrum:log_energy_band4\n",
      "raw_magnet:magnitude_spectrum:spectral_entropy\n",
      "watch_acceleration:magnitude_spectrum:log_energy_band0\n",
      "watch_acceleration:magnitude_spectrum:log_energy_band1\n",
      "watch_acceleration:magnitude_spectrum:log_energy_band2\n",
      "watch_acceleration:magnitude_spectrum:log_energy_band3\n",
      "watch_acceleration:magnitude_spectrum:log_energy_band4\n",
      "watch_acceleration:magnitude_spectrum:spectral_entropy\n",
      "watch_acceleration:spectrum:x_log_energy_band0\n",
      "watch_acceleration:spectrum:x_log_energy_band1\n",
      "watch_acceleration:spectrum:x_log_energy_band2\n",
      "watch_acceleration:spectrum:x_log_energy_band3\n",
      "watch_acceleration:spectrum:x_log_energy_band4\n",
      "watch_acceleration:spectrum:y_log_energy_band0\n",
      "watch_acceleration:spectrum:y_log_energy_band1\n",
      "watch_acceleration:spectrum:y_log_energy_band2\n",
      "watch_acceleration:spectrum:y_log_energy_band3\n",
      "watch_acceleration:spectrum:y_log_energy_band4\n",
      "watch_acceleration:spectrum:z_log_energy_band0\n",
      "watch_acceleration:spectrum:z_log_energy_band1\n",
      "watch_acceleration:spectrum:z_log_energy_band2\n",
      "watch_acceleration:spectrum:z_log_energy_band3\n",
      "watch_acceleration:spectrum:z_log_energy_band4\n"
     ]
    }
   ],
   "source": [
    "time_domain_keywords = ['mean', 'std', 'moment', 'percentile', 'entropy', 'magnitude_stats', '3d']\n",
    "frequency_domain_keywords = ['spectrum', 'log_energy_band', 'spectral_entropy', 'magnitude_spectrum']\n",
    "\n",
    "time_domain_columns = [col for col in feature_names if any(keyword in col for keyword in time_domain_keywords)]\n",
    "frequency_domain_columns = [col for col in feature_names if any(keyword in col for keyword in frequency_domain_keywords)]\n",
    "\n",
    "# Display the identified columns\n",
    "print(\"Time-Domain Columns:\")\n",
    "for col in time_domain_columns:\n",
    "    print(col)\n",
    "\n",
    "print(\"\\nFrequency-Domain Columns:\")\n",
    "for col in frequency_domain_columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = np.corrcoef(X175, rowvar=False)\n",
    "threshold = 0.95\n",
    "columns_to_remove = np.where(np.abs(corr_matrix) > threshold)\n",
    "columns_to_remove = [(i, j) for i, j in zip(columns_to_remove[0], columns_to_remove[1]) if i != j]\n",
    "dedup_columns = list(set([col[0] for col in columns_to_remove]))\n",
    "X175 = np.delete(X175, dedup_columns, axis=1)\n",
    "# X175 = X175[:,dedup_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select body state label\n",
    "# target_label = ['LYING_DOWN', 'SITTING', 'FIX_walking', 'FIX_running', 'BICYCLING', 'OR_standing']\n",
    "target_labels = ['LYING_DOWN', 'SITTING', 'FIX_walking']\n",
    "# Use the last 5 user's data as test set\n",
    "#test_uuid = list(range(56, 61))\n",
    "\n",
    "# Fill the Nan with mean value and normalize all the data \n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'mean')),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Transform \n",
    "# 1. select target labels \n",
    "# 2. tansform feature matrix fill None with mean and do the normalization\n",
    "# 3. Split train, validation and test set by ratio of 6:2:2\n",
    "# X_new, y_new, M_new = select_target_labels(X175, y, M, target_label, label_names, drop_all_zero = False)\n",
    "# X_new = pipeline.fit_transform(X_new, y_new)\n",
    "# Split the data into training, validation, and test sets\n",
    "# X_train_val, X_test, y_train_val, y_test, M_train_val, M_test = train_test_split(X_new, y_new, M_new, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val, M_train, M_val = train_test_split(X_train_val, y_train_val, M_train_val, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To swap 0 and 1, simply do \n",
    "# y_train_sampleweight = np.abs(1-M_train)\n",
    "\n",
    "# y_train_sw1D = np.zeros(y_train_sampleweight.shape[0])\n",
    "# for i in range(len(y_train_sw1D)):\n",
    "#     y_train_sw1D[i] = np.sum(y_train_sampleweight[i])/M_train.shape[1]\n",
    "\n",
    "# # To swap 0 and 1, simply do \n",
    "# y_test_sampleweight = np.abs(1-M_test)\n",
    "\n",
    "# y_test_sw1D = np.zeros(y_test_sampleweight.shape[0])\n",
    "# for i in range(len(y_test_sw1D)):\n",
    "#     y_test_sw1D[i] = np.sum(y_test_sampleweight[i])/M_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.shape, X_test.shape)\n",
    "# print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn_model(X_train, X_test, y_train, y_test, M_train, M_test, y_train_sw1D):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_shape=(X_train.shape[1],), activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    callback = EarlyStopping(monitor='loss',\n",
    "                                patience=3)\n",
    "    model.fit(X_train, y_train,\n",
    "                        epochs = 20, \n",
    "                        batch_size = 5,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        callbacks=[callback],\n",
    "                        verbose=0,\n",
    "                        sample_weight = y_train_sw1D)\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # print(classification_report(y_test, (y_pred > 0.5).astype(int)))\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, X_test, y_test)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_model(X_train, X_test, y_train, y_test, M_train, M_test, y_train_sw1D):\n",
    "    X_train_1 = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test_1 = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(1, input_shape=(1, X_train_1.shape[2]), activation=\"sigmoid\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.compile(loss = \"binary_crossentropy\",\n",
    "                optimizer = 'sgd',\n",
    "                metrics = ['accuracy'])\n",
    "    callback = EarlyStopping(monitor='loss',\n",
    "                                patience=3)\n",
    "    history = model.fit(X_train_1, y_train,\n",
    "                    epochs = 20, \n",
    "                    batch_size = 5,\n",
    "                    validation_data = (X_test_1, y_test), \n",
    "                    callbacks=[callback],\n",
    "                    verbose=0,\n",
    "                    sample_weight = y_train_sw1D)\n",
    "    # Store the performance metric for this split\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, X_test_1, y_test)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest_model(X_train, X_test, y_train, y_test, M_train, M_test, y_train_sw1D):\n",
    "    rf = RandomForestClassifier(class_weight = 'balanced', n_estimators = 10, min_samples_split = 10)\n",
    "    rf.fit(X_train, y_train, sample_weight = y_train_sw1D)\n",
    "    accuracy, precision, recall, f1 = evaluate_model(rf, X_test, y_test)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm_model(X_train, X_test, y_train, y_test, M_train, M_test, y_train_sw1D):\n",
    "    svm = SVC(kernel='linear', random_state=42)\n",
    "    history = svm.fit(X_train, y_train, sample_weight = y_train_sw1D)\n",
    "    # y_pred = svm.predict(X_test)\n",
    "    accuracy, precision, recall, f1 = evaluate_model(svm, X_test, y_test)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_model = {\n",
    "    'rnn': run_rnn_model,\n",
    "    'lstm': run_lstm_model,\n",
    "    'random_forest': run_random_forest_model,\n",
    "    'svm': run_svm_model\n",
    "}\n",
    "models = ['rnn', 'lstm', 'random_forest', 'svm']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(models, values, metric_name):\n",
    "\n",
    "    # Create the bar graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(models, values, color='skyblue')\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(metric_name)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Values')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for target_label in target_labels:\n",
    "    accuracy_scores = {}\n",
    "    precision_scores = {}\n",
    "    recall_scores = {}\n",
    "    f1_scores = {}\n",
    "    X_new, y_new, M_new = select_target_labels(X175, y, M, target_label, label_names, drop_all_zero = False)\n",
    "    X_new = pipeline.fit_transform(X_new, y_new)\n",
    "    for train_index, test_index in tscv.split(X_new, y_new):\n",
    "        # Split the data into training and test sets based on the current split indices\n",
    "        X_train, X_test = X_new[train_index], X_new[test_index]\n",
    "        y_train, y_test = y_new[train_index], y_new[test_index]\n",
    "        M_train, M_test = M_new[train_index], M_new[test_index]\n",
    "        # To swap 0 and 1, simply do \n",
    "        y_train_sampleweight = np.abs(1-M_train)\n",
    "        y_train_sw1D = np.zeros(y_train_sampleweight.shape[0])\n",
    "        for i in range(len(y_train_sw1D)):\n",
    "            y_train_sw1D[i] = np.sum(y_train_sampleweight[i])/M_train.shape[1]\n",
    "        \n",
    "        for model in models:\n",
    "            accuracy, precision, recall, f1 = run_model[model](X_train,X_test,y_train,y_test,M_train,M_test,y_train_sw1D)\n",
    "            accuracy_scores[model] = accuracy_scores[model]+[accuracy] if accuracy_scores.get(model) else [accuracy]\n",
    "            precision_scores[model] = precision_scores[model]+[precision] if precision_scores.get(model) else [precision]\n",
    "            recall_scores[model] = recall_scores[model]+[recall] if recall_scores.get(model) else [recall]\n",
    "            f1_scores[model] = f1_scores[model]+[f1] if f1_scores.get(model) else [f1]\n",
    "            \n",
    "        # callback = EarlyStopping(monitor='loss',\n",
    "        #                                         patience=3)\n",
    "        \n",
    "        # rnn_model = run_model['rnn'](X_train.shape[1])\n",
    "        # history = rnn_model.fit(X_train, y_train,\n",
    "        #                 epochs = 20, \n",
    "        #                 batch_size = 5,\n",
    "        #                 validation_data = (X_test, y_test),\n",
    "        #                 callbacks=[callback],\n",
    "        #                 verbose=0,\n",
    "        #                 sample_weight = y_train_sw1D)\n",
    "        # # Store the performance metric for this split\n",
    "        # append_score_to_model('rnn', [np.mean(history.history['val_accuracy'])])\n",
    "        # y_pred = rnn_model.predict(X_test)\n",
    "        # # print(classification_report(y_test, (y_pred > 0.5).astype(int)))\n",
    "        \n",
    "\n",
    "\n",
    "        # X_train_1 = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        # X_test_1 = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "        # lstm_model = run_model['lstm'](X_train_1.shape[2])\n",
    "        # history = lstm_model.fit(X_train_1, y_train,\n",
    "        #                 epochs = 20, \n",
    "        #                 batch_size = 5,\n",
    "        #                 validation_data = (X_test_1, y_test), \n",
    "        #                 callbacks=[callback],\n",
    "        #                 verbose=0,\n",
    "        #                 sample_weight = y_train_sw1D)\n",
    "        # # Store the performance metric for this split\n",
    "        # append_score_to_model('lstm', [np.mean(history.history['val_accuracy'])])\n",
    "\n",
    "\n",
    "        # rf_model = run_model['random_forest']()\n",
    "        # rf_model.fit(X_train, y_train, sample_weight = y_train_sw1D)\n",
    "        # append_score_to_model('rf', [rf_model.score(X_test, y_test)])\n",
    "\n",
    "\n",
    "        # svm_model = run_model['svm']()\n",
    "        # history = svm_model.fit(X_train, y_train, sample_weight = y_train_sw1D)\n",
    "        # y_pred = svm_model.predict(X_test)\n",
    "        # append_score_to_model('svm', [svm_model.score(X_test, y_test)])\n",
    "        # # print(classification_report(y_test, (y_pred > 0.5).astype(int)))\n",
    "\n",
    "\n",
    "    # Calculate the mean and standard deviation of the performance metric across all splits\n",
    "    \n",
    "    plot_metric(models, [np.mean(val) for val in accuracy_scores.values()], f'Accuracy for {target_label}')\n",
    "    plot_metric(models, [np.mean(val) for val in precision_scores.values()], f'Precision for {target_label}')\n",
    "    plot_metric(models, [np.mean(val) for val in recall_scores.values()], f'Recall for {target_label}')\n",
    "    plot_metric(models, [np.mean(val) for val in f1_scores.values()], f'F1 for {target_label}')\n",
    "    \n",
    "# fit network\n",
    "# model = model_1.fit(X_train, y_train,\n",
    "#                       epochs = 10, \n",
    "#                       batch_size = 16,\n",
    "#                       validation_data = (X_test, y_test), \n",
    "#                       sample_weight = y_train_sw1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy\n",
    "#plt.plot(score_rnn)\n",
    "# font = {'family' : 'DejaVu Sans',\n",
    "#         'weight' : 'normal',\n",
    "#         'size'   : 28}\n",
    "# plt.rc('font', **font)\n",
    "# fig = plt.figure(figsize = (16, 10))\n",
    "# score_1 = [0.6319, 0.6612, 0.6808, 0.6891, 0.6992, 0.6936, 0.6981, 0.6944, 0.6956, 0.6979]\n",
    "# score_2 = [0.6103, 0.7102, 0.7117, 0.7176, 0.7168, 0.7174, 0.7192, 0.7241, 0.7248, 0.7126]\n",
    "# plt.plot(score_1)\n",
    "# plt.plot(score_2)\n",
    "# plt.ylabel('Balanced Accuracy')\n",
    "# plt.title(\"Accuarcy Comparison\")\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend([\"MLP\", \"LSTM\"], loc = 'bottom right')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [samples, time_steps, features]\n",
    "X_train_1 = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_1 = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 942us/step - accuracy: 0.8648 - loss: 0.4801 - val_accuracy: 0.9351 - val_loss: 0.1644\n",
      "Epoch 2/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 889us/step - accuracy: 0.9339 - loss: 0.3723 - val_accuracy: 0.9485 - val_loss: 0.1390\n",
      "Epoch 3/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 972us/step - accuracy: 0.9398 - loss: 0.3931 - val_accuracy: 0.9530 - val_loss: 0.1316\n",
      "Epoch 4/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 990us/step - accuracy: 0.9534 - loss: 0.2080 - val_accuracy: 0.9597 - val_loss: 0.1272\n",
      "Epoch 5/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 908us/step - accuracy: 0.9534 - loss: 0.3023 - val_accuracy: 0.9620 - val_loss: 0.1268\n",
      "Epoch 6/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 841us/step - accuracy: 0.9422 - loss: 0.3397 - val_accuracy: 0.9642 - val_loss: 0.1199\n",
      "Epoch 7/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 855us/step - accuracy: 0.9494 - loss: 0.3146 - val_accuracy: 0.9664 - val_loss: 0.1151\n",
      "Epoch 8/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 842us/step - accuracy: 0.9541 - loss: 0.2291 - val_accuracy: 0.9664 - val_loss: 0.1095\n",
      "Epoch 9/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 942us/step - accuracy: 0.9571 - loss: 0.2168 - val_accuracy: 0.9709 - val_loss: 0.1045\n",
      "Epoch 10/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 903us/step - accuracy: 0.9642 - loss: 0.2289 - val_accuracy: 0.9754 - val_loss: 0.1045\n",
      "Epoch 11/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 909us/step - accuracy: 0.9677 - loss: 0.2018 - val_accuracy: 0.9776 - val_loss: 0.1011\n",
      "Epoch 12/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 800us/step - accuracy: 0.9626 - loss: 0.3050 - val_accuracy: 0.9754 - val_loss: 0.0973\n",
      "Epoch 13/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 892us/step - accuracy: 0.9672 - loss: 0.2069 - val_accuracy: 0.9776 - val_loss: 0.0922\n",
      "Epoch 14/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 853us/step - accuracy: 0.9695 - loss: 0.1581 - val_accuracy: 0.9732 - val_loss: 0.0937\n",
      "Epoch 15/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 863us/step - accuracy: 0.9633 - loss: 0.2254 - val_accuracy: 0.9754 - val_loss: 0.0976\n",
      "Epoch 16/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 913us/step - accuracy: 0.9627 - loss: 0.2731 - val_accuracy: 0.9732 - val_loss: 0.0915\n",
      "Epoch 17/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 970us/step - accuracy: 0.9696 - loss: 0.1872 - val_accuracy: 0.9754 - val_loss: 0.0980\n",
      "Epoch 18/20\n",
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 786us/step - accuracy: 0.9634 - loss: 0.3633 - val_accuracy: 0.9732 - val_loss: 0.0959\n",
      "Epoch 19/20\n",
      "\u001b[1m1567/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 578us/step - accuracy: 0.9696 - loss: 0.2292"
     ]
    }
   ],
   "source": [
    "hidden_size = 1\n",
    "time_step = 1\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(LSTM(1, input_shape=(time_step, X_train_1.shape[2]), activation=\"sigmoid\"))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = 'sgd',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# fit network\n",
    "model_lstm = model_2.fit(X_train_1, y_train,\n",
    "                         epochs = 20,\n",
    "                         batch_size = 1, \n",
    "                         validation_data = (X_test_1, y_test),\n",
    "                         sample_weight = y_train_sw1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/Checkout/Downloads/ExtraSensory-master/RNN_LSTM.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Checkout/Downloads/ExtraSensory-master/RNN_LSTM.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(scores[\u001b[39m'\u001b[39m\u001b[39mrnn\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mRNN\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Checkout/Downloads/ExtraSensory-master/RNN_LSTM.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(model_lstm\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mRNN with LSTM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Checkout/Downloads/ExtraSensory-master/RNN_LSTM.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mAccuarcy Comparison\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(scores['rnn'], \"RNN\")\n",
    "plt.plot(model_lstm.history['val_acc'], \"RNN with LSTM\")\n",
    "plt.title(\"Accuarcy Comparison\")\n",
    "plt.ylabel('Balanced Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['RNN', 'RNN with LSTM'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot training & validation accuracy\n",
    "# #plt.plot(score_rnn)\n",
    "# list_lstm = model_lstm.history['val_acc'] + [0.6942]\n",
    "# list_rnn = model.history[\"acc\"]\n",
    "\n",
    "# fig = plt.figure(figsize = (16, 10))\n",
    "# plt.plot(model.history['acc'])\n",
    "# plt.plot(list[1:])\n",
    "# plt.ylabel('Balanced Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(model_rnn.history['val_acc'], \"RNN\")\n",
    "# plt.plot(model_lstm.history['val_acc'], \"RNN with LSTM\")\n",
    "# plt.title(\"Accuarcy Comparison\")\n",
    "# plt.ylabel('Balanced Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['RNN', 'RNN with LSTM'], loc = 'upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
